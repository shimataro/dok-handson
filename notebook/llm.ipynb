{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406d0f70",
   "metadata": {},
   "source": [
    "# [高火力 DOK](https://www.sakura.ad.jp/koukaryoku-dok/) ハンズオン 〜LLM編〜\n",
    "\n",
    "## ステップ1: このファイルをJupyterLabで読み込む\n",
    "\n",
    "1. 右上↗️にある \"Raw\" をクリック（このファイルのソースが表示される）\n",
    "1. 「高火力 DOK」のノートブック機能でJupyterLabを起動\n",
    "1. \"File\" → \"Open from URL…\" を選択\n",
    "1. URL入力欄に、手順1で表示したソースのURLを貼り付け\n",
    "\n",
    "このファイルがJupyterLabに読み込まれたら、ステップ1は終了です。続いて下のステップ2に進んでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490dfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff089e",
   "metadata": {},
   "source": [
    "## ステップ2: プロンプトを調整\n",
    "\n",
    "下記ソースコードの `prompt` 変数を好きなメッセージに変更してください。\n",
    "\n",
    "好きなものに変えて構いませんが、チャットのような会話のキャッチボールをしたい場合は `[INST]` / `[/INST]` の部分は消さないでください。\n",
    "これを消すと、会話ではなく入力された文字列に続く文章が生成されます。\n",
    "たとえば「日本は」という文字列だけにすると、「日本はなんとかかんとか」みたいな文章が出てきます。\n",
    "\n",
    "`prompt` を変えたら、ステップ2は終了です。続いて下のステップ3に進んでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# モデル名\n",
    "model_name = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
    "\n",
    "# モデル名に対応したトークナイザーとモデルを読み込み（日本語対応）\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else \"auto\",\n",
    ")\n",
    "\n",
    "# ELYZA指定のフォーマットに合わせたプロンプト\n",
    "prompt = \"\"\"[INST] 東京スカイツリーについて簡単に教えて。 [/INST]\"\"\"\n",
    "\n",
    "# トークナイズ\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 推論\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# 結果表示\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a2f27",
   "metadata": {},
   "source": [
    "## ステップ3: 文書生成！\n",
    "\n",
    "上部のツールバーから⏩をクリックしてください。\n",
    "（\"Restart Kernel?\" のアラートが出たら \"Restart\" を押してOK）\n",
    "\n",
    "依存パッケージやチェックポイントなどのダウンロードなどでしばらく時間がかかります。\n",
    "ダウンロードが終わったらいよいよ画像生成が始まり、質問に対する応答（または続きの文章）が出てきます。\n",
    "\n",
    "もしかしたらトンチンカンな答えが返ってくるかもしれませんが、それがハルシネーション（幻覚）です。\n",
    "\n",
    "以上でハンズオンは終了です。お疲れ様でした！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec4405",
   "metadata": {},
   "source": [
    "## おまけ\n",
    "\n",
    "<https://github.com/shimataro/llm-chat> に、もう少し肉付けしたアプリケーションを置いています。\n",
    "\n",
    "- 一度応答したら終了ではなく、実際のチャットのように繰り返し質問できる\n",
    "- 文章全体が生成されるまで待たされるのではなく、生成される過程もストリーム表示される\n",
    "- テキスト生成だけでなく、Seq2Seqモデル（翻訳やText2Text）も使える\n",
    "- 認証が必要なモデルも使える\n",
    "\n",
    "モデルへの問い合わせなど基本部分は今回の内容と同じなので、ぜひ中を覗いてみてください。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
